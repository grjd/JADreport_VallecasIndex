
\documentclass[preprint,12pt]{elsarticle}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\usepackage{longtable}
%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%\usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Journal of Alzheimer’s Disease Reports}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{The Vallecas Index: selecting the most important self-assessed features for conversion to Mild Cognitive Impairment with Random Forests.}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Jaime Gómez-Ramírez}
\author{Marina Ávila-Villanueva}
\author{Miguel Ángel Fernández-Blázquez}

\address{Fundaci\'on Reina Sof\'ia \\     Centre for Research in Neurodegenarative Diseases (Fundaci\'on CIEN) \\ \emph{Valderrebollo, 5, 28031 Madrid, Spain}}


\begin{abstract}
Alzheimer’s Disease (AD) is a complex, multifactorial and comorbid condition. The asymptomatic behavior in the early stages makes the identification of the disease onset particularly challenging.
 %The asymptomatic behavior in early stages of the disease is a paramount obstacle to formulate a preclinical and predictive model of AD. 
Mild cognitive impairment (MCI) is an intermediary stage between the expected decline of normal aging and the pathological decline associated to dementia. The identification of risk factors for MCI is thus sorely needed. 
The widespread use of smartphones and other smart devices able to collect lifestyle's information is expected to foster a more participatory and personalized treatment of chronic diseases. Self-reported data -variables that can be reported by the subject herself e.g. age, income level, education, sleep, diet, physical exercise etc.- are called to play a key role in the promotion of healthier choices, promoting patients empowerment. 
%Self-reported data are  
%Self-reported features (easily accessible and low cost) ranked in importance relative to conversion to MCI has not only value as a predictor of conversion in asymptomatic stages, most importantly they may help in the design of personalized interventions informed by the subject’s lifestyle.
In this study we leverage on \emph{The Vallecas Project}, a large longitudinal Spain based study on healthy aging, to build \emph{The Vallecas Index}. \emph{The Vallecas Index}, via machine learning techniques (random forest), defines a set of most important self-reported variables for MCI conversion which includes subjective cognitive decline, educational level, working experience, social life and diet.
\end{abstract}

\begin{keyword}
Mild cognitive impairment \sep  Alzheimer's disease \sep Subjective cognitive decline \sep Machine learning \sep Random forest \sep Feature importance 
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
%\linenumbers

%% main text
\section{Introduction}
\label{se:intro}

Mild cognitive impairment (MCI) is an intermediary stage between the expected decline of normal aging and the pathological decline associated to dementia. The type of MCI associated with memory loss is called amnesic MCI. In non-amnesic MCI, memory function is not compromised but other cognitive abilities, e.g. language, executive functioning or spatial navigation skills, are impaired. MCI cases  compared with cognitively normal cases have a higher risk of progressing to dementia \cite{flicker1991mild}, \cite{petersen1999mild}, \cite{bruscoli2004mci}, \cite{buratti2015markers}, \cite{michaud2017risk}. Although the diagnosis of MCI has prognostic value, the value is intrinsically linked to the time of the diagnosis. 
The early detection of MCI is a priority target in aging research \cite{prince2018world}.

There is not only a concern with the early detection of cognitive decline symptoms, it is also important to understand the time stability of the MCI diagnosis \cite{Han2012}, \cite{Ellendt2017}. The \emph{yo-yo effect} refers to fluctuations between normal and MCI diagnosis observed for the same person. The variability in symptoms may lead to spurious diagnosis, for example, a neurologist or a neuropsychologist may diagnose a person with MCI and later in the future retract the diagnosis \cite{Zonderman2013}. 

In order to better understand this variability we need to take into account a number of factors that may have a measurable effect in the cognitive performance. MCI is, in the end, a clinical description based on performance on tests of memory and thinking skills. The capacity to perform well on a test can be affected by life style conditions. For example, a person going through stressful events, sleep deprived or with a poor diet may have worsening memory test scores compared to previous occasions with more favorable circumstances \cite{vecsey2015effects}, \cite{loewenstein2018utilizing}, \cite{sandi2007stress}, \cite{neupert2006daily}.

Testing memory and thinking skills in a large elderly population and in regular basis, rather than when memory loss starts to occur, might help us understand the role played by fluctuations between normal and MCI conditions in the risk of later developing dementia. The systematic examination, via for example cognitive testing, brain imaging techniques or gene expression profiling, of a very large of pool subjects is in all cases extremely costly. Prior or in addition to such an effort it is worth collecting information variables that can be directly assessed by the individuals and that could have an effect on cognitive impairment, such as life style, demographic, diet, sleep patterns or subjective memory complaints. 

In this work we use the dataset collected in \emph{The Vallecas Project}, the most ambitious longitudinal population-based study for healthy aging in Spain. For the reasons just discussed, we focus on features \footnote{The terms feature and variable are here used indistinctly. The former is the usual denomination in  machine learning and the last in statistics.} collected in \emph{The Vallecas Project} that can be self assessed by the participants and within this set of variables. Self-assessed features are variables that can be reported by the subject herself e.g. age, income level, education, sleep, diet, physical exercise etc.

We study the most important self-reported features for MCI conversion using statistical and machine learning techniques. The methodology of this feature selection problem is described in Section \ref{se:mandm} wherein Random Forest, the machine learning technique of choice for the study of feature importance is explained.
% together with alternative approaches for feature selection, namely filter and recursive methods. 
Section \ref{se:res} describes \emph{The Vallecas Index} i.e. the selection of the most important self-assessed features for conversion to MCI in the \emph{The Vallecas Project} dataset. Finally, in the last Section \ref{se:dis}, we discuss the uses of machine learning techniques applied to lifestyle data for dementia prediction, with especial emphasis on the opportunities that this approach hold for patient self management and empowerment. 
%The study results are supported by the Supplementary Materials Section. 

\section{Materials and Methods}
\label{se:mandm}

\emph{The Vallecas Project} started in the end of 2011 with a cohort size of 1203 subjects aged 70 or older, 31 subjects of the initial cohort were diagnosed with AD in their first visit and were removed from the project. The number of active subjects has decreased across the years, 964 subjects came to the second visit, 865 the third visit, 773 the fourth visit, 704 the fifth visit and 509 to the sixth visit, the last yearly visited completed. At the time of writing (15/07/2019) the project is running the 7th and 8th visits.
The main objective of \emph{The Vallecas Project} is to elucidate the best combination of features  that are informative about developing cognitive impairment in the future. 
\emph{The Vallecas Project} dataset includes information about a wide range of factors including magnetic resonance imaging, genetic, demographic, socioeconomic, cognitive performance, subjective cognitive decline, neuropsychiatric disorders, cardiovascular, sleep, diet, physical exercise and self assessed quality of life. The subjects in each visit were diagnosed as healthy, mild cognitive impairment (MCI) or dementia.

%YS do the table
For this study and we focus on features that are self-assessed by the participants in \emph{The Vallecas Project} for the completed visits, that is form visits first to sixth. 
The features fall within the following categories: demographics, anthropometric, neuropsychiatric,  traumatic brain injury, cardiovascular, quality of life, engagement with the external world, physical exercise, social engagement, sleep, diet and subjective cognitive decline. Table \ref{tab:pvall} shows the types of self-assessed features collected in \emph{The Vallecas Project} and studied in this work. 
The complete description of the dataset is included in the Supplementary Materials \ref{tab:multicol}.

\begin{table}[h!]
  \begin{center}
   \caption{Feature types used in the Vallecas Index, each category contains several features.}
    \label{tab:pvall}
\begin{tabular}{ |p{6cm}||p{6cm}|  }
\hline
Demographics & Anthropometric\\
\hline
Neuropsychiatric & Diet\\
\hline
Cardiovascular & Quality of Life\\
\hline
Engagement External World & Physical Exercise\\
\hline
Social Engagement & Traumatic Brain Injury\\
\hline
Sleep & Subjective Cognitive Decline\\
\hline
%\caption{PV features}
\end{tabular}
\end{center}
\end{table}

%Demographics_s ['renta', 'numero_barrio', 'numero_distrito', 'sexo', 'nivel_educativo', 'anos_escolaridad', 'sdestciv', 'sdhijos', 'numhij', 'sdvive', 'sdocupac', 'sdresid', 'sdtrabaja', 'sdeconom', 'sdatrb', 'edad_visita1']
%PsychiatricHistory_s': ['depre', 'depre_ini', 'depre_num', 'depre_trat', 'ansi', 'ansi_ini', 'ansi_num', 'ansi_trat']
%Diet 'alaves','alcar', 'aldulc', 'alemb', 'alfrut', 'alhuev', 'allact', 'alleg','alpan', 'alpast', 'alpesblan', 'alpeszul', 'alverd'
%['hta', 'hta_ini', 'glu', 'lipid', 'tabac', 'tabac_cant', 'tabac_fin', 'tabac_ini', 'sp', 'cor', 'cor_ini', 'arri', 'arri_ini', 'card', 'card_ini', 'tir', 'ictus', 'ictus_num', 'ictus_ini', 'ictus_secu']
%'eqm06_visita1','eqm07_visita1', 'eqm81_visita1', 'eqm82_visita1', 'eqm83_visita1','eqm85_visita1', 'eqm09_visita1', 'eqm10_visita1', 'eq5ddol_visita1','valfelc2_visita1', 'eq5dsalud_visita1'
%'EngagementExternalWorld_s': ['a01', 'a02', 'a03', 'a04', 'a05', 'a06', 'a07', 'a08', 'a09', 'a10', 'a11', 'a12', 'a13', 'a14']
%SocialEngagement_s': ['relafami', 'relaamigo', 'relaocio_visita1', 'rsoled_visita1']
% scd = ['scd_visita1','act_aten_visita1', 'peorotros_visita1','act_orie_visita1',
%'act_mrec_visita1','act_expr_visita1','act_prax_visita1','act_ejec_visita1','act_comp_visita1',\
%    'act_visu_visita1','eqm06_visita1','eqm07_visita1','eqm81_visita1','eqm82_visita1','eqm83_visita1',\
%    'eqm84_visita1','eqm85_visita1','eqm86_visita1','eqm09_visita1','eqm10_visita1']
% %'scd_visita1','act_aten_visita1', 'peorotros_visita1', 'act_mrec_visita1','act_expr_visita1'
%YS; DELETE 'card', 'sdresid' 'a07', 'a14'

%REMOVED Features for Low variance:  Index(['sdhijos', 'act_orie_visita1', 'act_prax_visita1', 'act_ejec_visita1','act_comp_visita1', 'act_visu_visita1', 'eqm84_visita1','eqm86_visita1', 'eq5dmov_visita1'],dtype='object')

%YS: span multiple pages table
%https://tex.stackexchange.com/questions/165793/how-to-make-a-table-on-more-than-one-page-by-using-the-table-environment

\begin{table}[ht]
\caption{Self-assessed features collected in \emph{The Vallecas Project}}
\begin{center}
%\begin{longtable}{|l|l|l|}
\begin{tabularx}{\linewidth}{L L L}
	Type&Name&Description\\
    \hline
    \multirow{12}{*}{Demographics}
    &age&age year 1 $\mathbb Z_{\ge 0}$\\ %sdocupac YS
    &income&average income by zip code $\mathbb R_{> 0}$\\
    &sex&male or female\\ %sexo
    &educational level&[None, primary, secondary, university]\\ %nivel_educativo
    &years of schooling&$\mathbb Z_{\ge 0}$\\ %anos_escolaridad
    &marital status&single, married, widow, divorced\\ %sdestciv
    &sons and daughters&$\mathbb Z_{\ge 0}$\\ %numhij
    &household composition&[alone, in couple, with family, with carer]\\ %sdvive $\mathbb Z_{\ge 0}$
    &profession&$\mathbb Z_{\ge 0}$\\ %sdocupac YS
    &population residence&$\mathbb Z_{\ge 0}$\\ %sdresid YS
    &an employee&[0,1]\\ %sdresid YS
    &socio-econ.status&$\mathbb Z_{\ge 0 \le 10}$\\ %sdeconom YS 1,10
    &years an employee&$\mathbb Z_{\ge 0}$\\ %sdeconom YS 1,10
    \hline
    
    \multirow{7}{*}{Anthropometric}
    &lat-manual&right,left handed [0,1,2]\\
    &pabd&perimeter of the abdomen $\mathbb R_{> 0}${$cm$}\\
    &weight&weight year 1 $\mathbb R_{> 0}${$kg$}\\
    &height&height year 1 $\mathbb R_{> 0}${$m$}\\
    &audi&auditory deficit [0,1]\\
    &visual&Visual deficit [0,1]\\
    &bmi&body mass index year 1 $\mathbb R_{> 0}$\\
    \hline
    
    \multirow{2}{*}{Neuropsychiatric}
    &depression&suffered from depression [0,1]\\
    &anxiety&suffered from anxiety [0,1]\\
    \hline
    
    \multirow{12}{*}{Diet}
    &red-meat &consumption days/week [1-2,3-5,6-7]\\
    &sweets &days/week eat sweets [1-2,3-5,6-7]\\
    &charcuterie &days/week eat charcuterie [1-2,3-5,6-7]\\
    &white-meat &days/week eat white meat [1-2,3-5,6-7]\\
    &fruits &days/week eat fruits [1-2,3-5,6-7]\\
    &eggs &days/week eat eggs [1-2,3-5,6-7]\\
    &dairy &days/week eat dairy [1-2,3-5,6-7]\\
    &legumes &days/week eat legumes [1-2,3-5,6-7]\\
    &bread &days/week eat bread [1-2,3-5,6-7]\\
    &pasta &days/week eat pasta [1-2,3-5,6-7]\\
    &white-fish &days/week eat white fish [1-2,3-5,6-7]\\
    &blue-fish &days/week eat blue fish [1-2,3-5,6-7]\\
    &vegetables &days/week eat vegetables [1-2,3-5,6-7]\\
    \hline

    \multirow{6}{*}{Cardiovascular}
    &hbp&high blood pressure [0,1]\\ %hta
    &glucose&glucose metabolism [0,1,2]\\ %glu
    &dyslipidemia&dyslipidemia [0,1,2,3]\\ %lipid
    &tobacco&smoker now or past[0,1]\\ %tabac 
    &heart&no heart problem, angina, infarct [0,1,2]\\ %cor
    &arrythmia&No Arrhythmia, Atrial fibrillation, Arrhythmia [0,1,2]\\ %arri
    \hline
    
    \multirow{10}{*}{Quality of Life}
    %&mob to&today's mobility[1,2,3]\\ %eq5dmov_visita1 remobed for low variability
    &pain&today's pain[1,2,3]\\ %eq5ddol
    &happiness&today's happiness[1,2,3,4]\\ %valfelc2_visita1_visita1
    &health-cmp&well being compared with last year[1,2,3]\\ %eq5dsalud_visita1
    &mem-lo-how&how is memory loss (slowly,suddenly, DK/DA)  [1,2,3]\\ %eqm06_visita1 
    &mem-lo-rec&difficulty retaining recent info [0,1]\\ %eqm07_visita1
    &mem-lo-conv&memory loss affects remember recent conversations [0,1]\\ %eqm81_visita1
    &mem-lo-pp&memory loss affects remember people/places [0,1]\\ %eqm82_visita1
    &mem-lo-obj&memory loss affects remember objects names [0,1]\\ %eqm83_visita1
    &mem-lo-dai&memory loss affects daily activity [1,2,3,4,5]\\ %eqm10_visita1_visita1
    &mem-lo-obj-f&problems finding objects [0,1]\\ %eqm85_visita1
    &mem-lo-wri&wrote notes to cope with memory loss [1,2,3]\\ %eqm09_visita1
    \hline
    
    \multirow{12}{*}{Engagement External World}
    &eew-sport&frequency doing sports [1,2,3]\\ %a01
    &eew-recre&frequency doing recreational activities [1,2,3]\\ %a02
    &eew-friends&frequency going out friends [1,2,3]\\%a03
    &eew-travel&frequency travel/tourism [1,2,3]\\%a04
    &eew-ngo&frequency NGOs activities [1,2,3]\\%a05
    &eew-church&frequency church activities [1,2,3]\\%a06
    %&eew-social&frequency social club activities [1,2,3]\\%a07
    &eew-art&frequency art related (converts, expositions) [1,2,3]\\%a08
    &eew-sport-e&frequency sport events activities [1,2,3]\\%a09
    &eew-music&frequency listens to music [1,2,3]\\%a10
    &eew-tv&frequency TV/radio [1,2,3]\\%a11
    &eew-read&frequency read book/magazines [1,2,3]\\%a12
    &eew-it&frequency internet [1,2,3]\\%a13

    \hline
    \multirow{1}{*}{Physical Exercise}
    &phys&session $\times$ frequency {$min/week$}\\
    \hline

    \multirow{4}{*}{Social Engagement}
    &rel-friends&frequency see friends [1..5]\\%relaamigo 
    &rel-fami&freq. see family [1..5]\\ %relafami
    &rel-leis&freq. leisure outside [1,2,3]\\%'relaocio_visita1'
    &rel-lone&freq. felling alone [1,2,3]\\ %'rsoled_visita1'
    
    \hline
    \multirow{1}{*}{Traumatic Brain Injury}
    &tbi&episode(s) of TBI [0,1]\\
    \hline
    
    \multirow{11}{*}{Sleep}
    &sleep-dy&hrs. of diurnal sleep\\
    &{sleep-ni}&hrs. of nocturnal sleep [0,1]\\
    &{sleep-ti}&tickling while sleep [0,1]\\
    &{sleep-mv}&moves while sleep [0,1]\\
    &{sleep-dr}&dreams while sleep [0,1]\\
    &{sleep-de}&deep sleep [0,1]\\
    &{sleep-re}&remember dreams [0,1]\\
    &{sleep-en}&enough sleep [0,1]\\
    &{sleep-as}&problems to fall asleep [0,1]\\
    &{sleep-in}&interruptions while sleep [0,1]\\
    &{sleep-sn}&snores while sleep [0,1]\\
    \hline
    
    \multirow{5}{*}{Subjective Cognitive Decline}
    &scd&subjective cognitive decline $\mathbb Z_{\ge 0 \le 10}$\\ 
    &s-attention&self perceived loss attention loss [0,1]\\
    %'peorotros_visita1', 'act_mrec_visita1','act_expr_visita1'
    &s-worse-others&feeling doing worse than others [0,1]\\ 
    &s-attention&self perceived worsen memory [0,1]\\
    &s-lang&self perceived worsen language expression [0,1]\\ 
    \hline

\end{tabularx}
\end{center}
%\end{longtable}
\label{tab:multicol}
\end{table}

\subsection{Automated Feature Selection}
We are interested in studying the predictive power of self-assessed features collected in \emph{The Vallecas Project} on future conversion to mild cognitive impairment (MCI). Our goal is to study the most important features to predict conversion to MCI using Random Forests and compare the results with standard linear procedures. 

The goal of feature selection is to find an optimal set of features according to an evaluation metric. Depending on the evaluation metric, we can distinguish between two methodologies of automated feature selection, namely filter and embedded, both can be used to remove the non-essential features for the task of predicting new values of the target feature. 

The engine of this automatic feature selection problem has as input the set of self-assessed features (Table \ref{tab:multicol}) measured in year 1 and as output target, the conversion to MCI diagnosis in the latest available visit (year 6). Thus, a subject that came to only the first and second visits, the input is the set of self-assessed features in year 1 and the output is conversion to MCI in year 2. On the other hand, a subject that came to visits 1,2 and 6, the input is invariably the set of self-assessed features in year 1 and the output target is conversion in the last available year or year 6 in this case. %This is graphically represented in Figure \ref{}.

The problem of feature selection if properly tackled, needs to deal with at least three milestones, first \emph{How} many features can be included as a minimum set of important features, second \emph{Which} are the most important features, and third, \emph{Why} are those features the most important ones.  

% How many
\subsubsection{How many features? The one in ten rule}
The first question we must consider is, How much data is enough to consistently predict the target? The answer is not straightforward and depends on a number of factors, for example, the type of model (linear or non-linear), the accuracy we want to achieve, the quality of the data (signal-to-noise ratio), the number of inputs and so on. Arguably, the required size of the training data is an ill-posed question, however, data scientists have instead proposed heuristics to address this problem. One such heuristic is the \emph{one in ten} rule which states that the amount of data needed is 10 times the number of parameters in the model \cite{harrell2001reg}. For example, according to the \emph{one in ten} rule of thumb, in a sample of 1,000 subjects with 140 positive cases (e.g converted to MCI), 14 parameters, that is, the $10\%$ of the minority class) can be used to reliably fit the total dataset. Thus, for our dataset, the \emph{one in ten} would suggest selecting 14 from the initial YS self-assessed features shown in Table \ref{tab:pvall}. 

The \emph{one in ten} rule effectively transforms the problem of deciding the size of the training set by that of knowing the number of parameters in the model. In the case of linear models this is a trivial task since the number of parameters is equal to the number of inputs. 
Nevertheless, the \emph{one in ten} should be seen as a reasonable guess on the number of features and never as prerequisite.
% Which ones
\subsubsection{Which features?}
As important or more as deciding about the number of features to be included in the model is to be able to asses the relative importance of the features. In order to study which are the most important features for prediction accuracy, we need to discuss first the required methodology to estimate the usefulness of the features. Filter methods and embedded methods (random forests) are introduced next.

%However, by including all the input features in the model we may be introducing multicollineariy (correlations between input variables). 
%which is a problem because independent variables should be independent.
%Feature selection by regularization or other means that discards irrelevant or redundant features is a necessary step with clear benefits including: multicollinearity, reduction in complexity and increase in interpretability, reduction of overfitting and improved processing time.

%Wrapper methods estimate the usefulness of each feature before to include it in the classifier, only those features that are identified as useful are included all the rest are removed.  

% wrapper 
% \subsubsection{Filter method: SelectKBest}
Filter methods pick up the intrinsic properties of the features estimated via univariate correlation matrix. In essence, a filter method is a linear approach to finding variables that contain most information about the target variable by means of statistical tests (e.g. chi-squared test, Fisher's exact test) or related quantities such as the correlation coefficient.

% to determine, for example, the variables with variance above a certain threshold.
The algorithm \emph{SelectKBest} \cite{scikit-learn} uses a score function in order to remove all but the highest scoring features, that is to say, only the $k$ most important features will be retained. \emph{SelectKBest} can take as score function the Fisher's test, the $\tilde{\chi}^2$ and the mutual information. For example, the referred algorithm with $\tilde{\chi}^2$ as the score function selects the $k$ most important input features $X$ to predict the target feature $y$. A small value of the $\tilde{\chi}^2$ statistic for every par $x,y$ means that the feature $x$ is independent of $y$. On the other hand, a large value means that $x$ is non-randomly related to $y$, and so likely to provide important information about the target. 

%\subsubsection{Embedded method: Random Forests}
Random forests are one the most popular machine learning algorithms with a particularly good performance in small and medium size datasets, and most importantly for this study, random forest is the algorithm of choice for automated feature selection \cite{breiman2001random}. How ensemble methods like random forests work may be understood by using the analogy of asking to a thousand people (experts) and then aggregating their answers. Likely, the aggregated response is better than the individual expert’s responses. By the same token, the aggregation of many inaccurate predictors will give better answers than the individual predictions. Or going from analogy to allegory: \emph{Don’t look at the tree... but to the forest!.}
%Among the advantages of random forest compared to other machine learning techniques are good predictive performance, low overfitting and easy interpretability. 

A random forest consists of a large number of decision trees (tree-like model of decisions), where each tree is trained on \emph{bagged data} (sampling with replacement) using random selection of features \cite{trevor2009elements}. Thus, a random forest is in essence a meta estimator that fits a number of decision tree classifiers. 
Decision trees are nonparametric models\footnote{The term nonparametric model is a misnomer, it does not mean that the model does not have parameters, but that the number of parameters is not known in training time.}, that is to say, the model will have as many parameters as it needs to fit the data. It follows that if left unconstrained, the tree will fit the data very closely, most likely overfitting. Decision trees are unstable in the sense that small changes in the input may produce very different decision trees.
Random forests effectively address the overfitting and the stability problems existing in decision trees.

Random Forest do not suffer from the limitations of the beforementioned filter methods. Filter methods use correlation to assess the relevance of features, but they are likely to fail finding the best subset of features when features do not behave linearly (e.g. non-normality, multicollinearity or heterocedasticity \cite{ratkowsky1990handbook} exist in the data set). Thus, Random forests solve the real problem of feature selection because it tells us which are the most important features to optimize the prediction. Filter methods, on the other hand, do not solve any optimization problem, they rather tell us which features are most linearly correlated with the target feature. 

The important features in a decision tree are located in the nodes close to the root of the tree and the unimportant ones will tend to be close to the leaves of the tree or entirely absent from the tree. Therefore, random forests allow us to get an estimate of the importance of any feature by calculating how deep in the tree the feature appears across all the trees. 
Specifically, feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within Random Forest. 
%As a classifier, random forest performs an implicit feature selection, using a small subset of strong variables for the classification only, leading to its superior performance on high dimensional data.

Formally, the importance of feature $X_m$ for predicting target $Y$ is evaluated by adding up the weight impurity decreases, $\delta_i(s_t,t)$, for all nodes where $X_m$ is used, averaged over all trees $N_T$ in the forest.
%The risk of this procedure is a potential bias towards correlated predictive variables. 

The "Gini importance" -a computationally efficient approximation to the entropy- is a score that provides a relative ranking of the spectral features and is a by-product in the training of the random forest classifier, that is to say, the feature selection mechanism is embedded in the training algorithm of the classifier. 
This is roughly how it works, at each node $\tau$ within the binary trees T of the random forest, the optimal split is computed using the Gini impurity $i(\tau)$, measuring how well a potential split is separating the samples of the two classes in this particular node.

Formally, the decrease in the importance $\delta(i)$ from splitting and sending the sample to two sibling nodes of $\tau$, $\tau_a$ and $\tau_b$, with respective sample fractions $p_a=n_a/n$ and $p_a=n_b/n$ by a threshold $t_\theta$ on variable $\theta$ is defined as:
\begin{equation}
\delta(i)(\tau) = i(\tau) - p_{a}i(\tau_{a})- p_{b}i(\tau_{b})
\label{eq:delatiatu}
\end{equation}
where the the Gini impurity $i(\tau)$ is calculated as: 
\begin{equation}
i(\tau) = 1 -p_{1}^2 - p_{0}^2
\label{eq:iatu}
\end{equation}
where $p_k = \frac{n_k}{n}$ and $n_k$ is the number of samples from the class $k =\{0,1\}$, out of the total n samples.

Performing an exhaustive tree search over all variables $\theta$ available at the node, and over all possible thresholds $t_\theta$, the pair $\theta, t_{\theta}$ leading to a maximal decrease impurity $\delta(i)$ is determined. The decrease in Gini impurity resulting from this optimal split $\delta(i)_{\theta}(\tau, T)$ is recorded and accumulated for all nodes $\tau$ in all trees T in the forest. 

The Gini importance $I_G$ tells us how often any given feature $\theta$ was selected for a split, and how large its overall discriminative value was for the classification problem.

\begin{equation}
I_{G}(\theta) = \sum_T \sum_{\tau} \delta(i)_{\theta}(\tau,T)
\label{eq:gini}
\end{equation}


\subsubsection{How much important? Permutation Importance}
%YS: ojo lit
%https://christophm.github.io/interpretable-ml-book/feature-importance.html
Random forests can produce a ranking of the features based on its importance calculated as the mean decrease of Gini impurity \ref{eq:gini} although Gini impurity is not the only measure, Information Gain, Chi-square or entropy can also be used. The higher the score the more useful the feature is at correctly split the data at a given node.
From a more fundamental standpoint, the importance of a feature can be seen as the increase in the prediction error of the model after we permuted the feature’s values. By virtue of permuting the feature’s values the relationship between the feature and the true outcome is broken, now if the model error increases the feature is important because in this case the model relied on the feature for the prediction. By the same token, is a feature is unimportant, shuffling its values will likely leave the model error unchanged, because in this case the model ignored the feature for the prediction.

The permutation feature importance measurement, although initially introduced by Breiman \ref{breiman2001random} for random forests is model-agnostic and ca be used for any other fitted models \ref{fisher2018model}.
It is worth noting that permutation feature importance takes also into account the interaction effects on model performance because shuffling the feature values destroy the interaction effects with other features and with the  target.  

%YS

While feature importance shows what variables most affect predictions, partial dependence plots PDP show how a feature, internally, affects predictions, that is to say allows us to understand how changes in the range of values of the variable affect prediction. Partial Dependence Plots how a single feature impacts predictions. But there's a lot they don't show. For instance, what is the distribution of effects? Is the effect of having a certain value pretty constant, or does it vary a lot depending on the values of other features.  Is the effect of having a certain value pretty constant, or does it vary a lot depending on the values of other features.
%Like permutation importance, partial dependence plots are calculated after a model has been fit.

Finally, while permutation importance allows us to  make comparisons between features based on numeric measures of their effect on the predictions, it doesn't unequivocally  us how each features matter. For example, 
If a feature has medium permutation importance, that could mean it has a large effect for a few predictions, but no effect in general, or a medium effect for all predictions.

SHAP summary plots give us a birds-eye view of feature importance and what is driving it. SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations.
Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).
Shap values allow us to decompose any prediction into the sum of effects of each feature value yielding a graph like the "force". SHAP Dependence Contribution Plots: 


\section{Results}
\label{se:res}

We select the most important self-assessed features for conversion to MCI for subjects that have at least 2 visits (964 subjects). The number of subjects that converted to MCI is 156 subjects ($~17\%$ conversion rate). 


Figure \ref{fig:rf14} shows the results of the study of feature importance with the filter method \emph{SelectKBest} \cite{scikit-learn}. The score function used, $\tilde{\chi}^2$, gives the $\tilde{\chi}^2$ statistic between each input feature and the target feature. A large value will mean the input feature is non-randomly related to the target feature, and so likely to provide important information.

From the initial 92 features only 14 features are retained (\emph{one in ten} rule). The 14 most important features for conversion to MCI using \emph{SelectKBest} are shown in Figure \ref{fig:rf14}. The Pearson's between features is also shown (only when $>0.1$). The most highly correlated feature to conversion to MCI is use of information technologies ($r<-0.1$). 

The most important features according to the \emph{SelectKBest} algorithm are in this order: subjective cognitive decline, APOE\footnote{APOE is not a self-assessed feature but it provides valuable information and allows us to have an idea about the importance of lifestyle and self-assessed variables compared to genetic variables}, number of residents at home, age, use of IT, diet high in sweets, memory complaints for daily life tasks, body mass index, fast food fats, sweets, memory complaints to remembering object names, thyroid related problems, relationship with friends, schooling years, number of sons and daughters and smoking habits (present and past).  

\begin{figure}[!htb]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/inpaper/HeatmapTopF_chi2}
        \caption{Evaluation of the importance of features for conversion to MCI classification using SelectKBest. The 14 features most informative for MCI conversion are displayed in order of importance (first subjective cognitive decline, last smoking) in both axis. Conversion to MCI is displayed in the last row. The Pearson's correlation among all pair of features including the output target (conversion to MCI) are shown when $r>0.1$ } 
        \label{fig:rf14}
\end{figure}

%\textbf{Embedded method:Random forest}
%https://stats.stackexchange.com/questions/36165/does-the-optimal-number-of-trees-in-a-random-forest-depend-on-the-number-of-pred
%Random forests does not overfit. You can run as many trees as you want https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks
%https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html

Figure \ref{fig:RF_selectorgini} shows the 14 most informative features using Random forests. The importance of a feature is computed as the (normalized) total reduction of the Gini importance (the mean decrease impurity \ref{eq:gini}) brought by that feature \cite{breiman2017classification}. The higher the importance 
the more useful the feature, that is the, the better splits the data in the tree. 


The most important feature is subjective cognitive decline, followed by schooling years, working years, hours of night sleep, the self perceived economic status, number of sons and daughters, hours of diurnal sleep, relationship with friends, number of people living at home, diet high in sweets, age, diet rich in legumes and APOE.
There is a visible overlap in the important features using random forest and the linear \emph{SelectKBest} method. The most interesting difference worth discussing is that APOE in random forest drops in importance. In random forest sleeping patters also appear as among important features which were not captured in the linear method. 
Interestingly, the subjective cognitive decline is not only the most important feature (highest Gini score in the y-axis) it is also the feature with the shortest inter-tree variability (the random forest implementation shown contains 100,000 trees). The inter-tree variability is represented in the figure with black thin bars. 

Social isolation and loneliness is believed to be a risk factor in AD \cite{holmen1992loneliness}, \cite{fratiglioni2000influence}, \cite{shankar2013social}, \cite{holwerda2014feelings}, \cite{evans2018social}. A 2007 longitudinal study with up to 4 years of in-home follow up, reported a positive association between loneliness and an increased risk of late-life dementia \cite{wilson2007loneliness}. Poor sleep is linked to AD, in particular, the sleep–wake cycle has an effect on levels of amyloid-beta in the brain \cite{ju2014sleep}.

\begin{figure}[!htb]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/inpaper/rf_figu2}
        \caption{Evaluation of the importance of features for conversion to MCI classification.  The 14 features most informative are displayed in the x-axis, the y-axis shows the Gini coefficient. The black thin bars denote the inter-trees variability for a random forest implementation containing  100,000 trees.} 
        \label{fig:RF_selectorgini}
\end{figure}

\subsubsection{}
Recall captures opportunity cost, that is, the cost to pay for having a false negative (missing treatment) admittedly there is no treatment for MCI. Recall is important when the number of opportunities (positive cases which tend to be the minority class) is small, for example, if there are 10 positive cases with a 20 recall you will have only 2.
Thus, precision will be more important than recall when the cost of acting is high, but the cost of not acting is low.  Since the cost of passing up a sick patient is high, precision is less important than recall. If you want to hire and there are many candidates you want precision, because there are many opportunities, buying candidates are low cost.

Flu shot (Recall) vs Satellite launching (Precision).

Recall is more important when the opportunity cost of passing up a positive case is high. For example, flu  shot: give the flu shot to someone does not need it and it will cost you only few dollars, but do not give it to someone who needs it and may die. Because of this, health care plans offer flu shot to everyone, disregarding precision entirely.

Precision is more important in the satellite launching case. Let us say we say is a bad weather day and should not launch the satellite (false negative), no big deal we can launch it any other day. But if we predict good conditions and instead is bad up there (false positive), the satellite will get destroyed. In this case, precision is better than recall.

%PREcision is to PREgnancy tests as reCALL is to CALL center.
%https://medium.com/datadriveninvestor/accuracy-trap-pay-attention-to-recall-precision-f-score-auc-d02f28d3299c
Another way of seeing is what can you tolerate. Can you tolerate saying a healthy person that is sick (false positive) or you rather prefer to miss a sick person and tell him is healthy (false negative). False negative are worse because without treatment may die, while in the false positive people will get worried unnecessarily, that is all. False negative are not tolerable in predicting curable diseases.
Recall tells us the prediction accuracy among only actual positives.It means how correct our prediction is among ill people. That is why we have to minimize false negatives which means we are trying to maximize recall.
When “false positives” can not be tolerated, precision should be favoured (spam).

F score is an harmonic mean between P and R. F score is an alternative. 
ROC is just connecting the dots of FPR and TPR for different threshold values. The ROC tells us how good is the model separating the two classes and also tells us about choosing the threshold.
%AUC = 0,5 means that your model seperates two possible outcomes randomly. AUC can be 1.0 maximum which means perfect seperation.
TPR = Recall
FPR = FP/(TN + FP)

If TP =0 then both precision and recall are 0, because P = TP/(TP+FP) and R=TP/(TP+FN)

\section{Discussion}
\label{se:dis}

There is not an iron law of progression from normal to MCI to dementia, some people with MCI might never get worse, and a few would even get eventually better \cite{avila2017subjective}. The identification of the most important features in the dataset is a powerful interpretation tool with obvious benefits in prevention and diagnostics. 

In this work we have selected the subset of most important self assessed features for MCI conversion. This set of ranked variables defines \emph{The Vallecas Index}. 
\emph{The Vallecas Index} identifies the set of the most important features for conversion to MCI using \emph{The Vallecas Project} Dataset. 

The rationale and motivation behind \emph{The Vallecas Index} is twofold. First, to reduce the complexity of the problem at hand in order to gain in interpretability. The \emph{Vallecas Project} dataset has thousands of variables and having irrelevant features can decrease the accuracy of the model. Furthermore, it is not enough having a model that provides accurate predictions, being able to interpret the predictions made is essential, in particular in a clinical domain.
% especially linear algorithms like linear and logistic regression. 
Second, by detecting life style and other self assessed features related to MCI conversion, it is possible to provide recommendations and to suggest healthy-aging effective choices.

%It is however crucial to understand which and how many features are relevant for the problem at hand, which in our case is prediction of MCI. A model with too many parameters may not generalize well from our training data to unseen data, this is called overfitting. By the same token, a model that is too simple with regards to the data it is trying to model is said to underfit \cite{goodfellow2016deep}.

\emph{The Vallecas Index} was obtained via Random Forest -an ensemble learning method for classification. Random forests are one of the most used machine learning techniques, among the reasons of its popularity are accuracy, robustness and most importantly for this study, random forest have built-in feature selection capabilities. 
%Non linear models (e.e. Random forests) do not need to deal with the problem of multicollinearity (independent variables in a regression model are correlated) since they do not rely upon the assumption that the input variables are independent as is the case in linear models (e.g. logistic regression). However, feature selection needs to be performed even more so. 

A biomarker is a measurable indicator of some biological state. This same idea can be carefully extended to identify lifestyle factors that influence the deterioration of cognitive performance and ultimately the development of AD \cite{ellis2010addressing}. The widespread use of smartphones and other smart devices able to collect lifestyle's information is expected to foster a more participatory and personalized treatment of chronic diseases. Self-reported data are called to play a key role in the promotion of healthier choices, promoting patients empowerment. 

The application of automated feature selection algorithm with Random Forest to self-assessed variables of \emph{The Vallecas Project} dataset shows that the subjective cognitive decline together with the professional and educational level, loneliness and social life, sleep pattern and diet are the most important features for conversion to MCI in healthy elderly subjects.

The unfulfilled promise of disease-modifying therapies for Alzheimer disease suffers from the lack of a theory of aging able to explain the dynamic progression of both normal and pathological aging \cite{cerella1985information}, \cite{mangel2001complex}, \cite{sleimen2014aging}, \cite{cohen2016complex}.
%jeremy english https://www.quantamagazine.org/first-support-for-a-physics-theory-of-life-20170726/
While a theory of biological aging, able to explain the very complex intricacies and variability of aging may seem today quixotic if not far-fetched or impossible, the ever-increasing availability of Big Data coupled with data analytics is challenging research practices. 

The epistemological implications of the data-drive approach of machine learning are at this point still unclear, however, what is incontestable is that research in problems lacking a clear theoretical framework such as chronic diseases, will bring a more intensive (never less) use of data and predictive analytics. Building prediction machines for conversion to MCI using self-informed features, i.e. variables that can be reported by the subject herself, represents a step forward towards a new medicine that aspires to be closer to the 3 Ps: \emph{personalized, preventive} and \emph{precise}. 


%\LaTeX{} \cite{latex2e} is a set of macros built atop \TeX{} \cite{texbook}.
\bibliographystyle{model1-num-names}
\bibliography{bibliojgr}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}



\end{document}


%There is a risk of overfitting in non-linear models e.g. artificial neural networks with a large number of parameters . The one in ten rule can be relaxed and used as a lower bound to the amount of training data needed. Thus, using the same example, at least 14 parameters can be reliably fitted to the total dataset using a non linear model.




%Here we address this feature selection problem is addressed here using machine learning techniques. 
% Furthermore, feature selection helps reducing the complexity of the statistical model or machine learning model.
% The incidence of Alzheimer’s disease grows exponentially with age. 
% Subjective cognitive decline REFS MA-MA %\ref{}{}. 

%Embedded methods use a learning algorithm to select the optimum set of features. Random forest is the most commonly used embedded method for feature selection  . 
%Embedded methods optimize the classifier performance, and most importantly the evaluation metric is built in the model during the learning process. 
%A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).
%Wrapper methods, contrary to filter methods, measure the usefulness of a subset of features by actually training a model on it. On the other hand, wrapper methods compared to filter methods are computationally very expensive. Wrapper methods, are similar to 
%Embedded methods optimize the classifier performance, and most importantly the evaluation metric is built in the model during the learning process.
%A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).
%We perform automated feature selection using filter and embedded method (Random Forest). Additionally, results obtained with wrapper methods are shown in the Supplementary Materials \ref{}. 
%Here we provide a working definition of random forest, a more ind detail description is given in Supplementary Materials. 
%YS Table
%YS Dec Tree vs ttest 

%which is defined as the most important self-assessed features  for subjects that have at least 2 visits (920 subjects). 

% The number of subjects that converted to MCI is 156 subjects. 
% The dataset of the \emph{Vallecas Project} used here is of the order of 1,000 samples and can arguably be considered to fall within the small-medium class category. 
% DISTIMGUISH BETWEEN RISK AND SHANNON INFORMATION, WE DO THE LAST

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:
\newpage
%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\section{Supplementary materials}

The advantage of Random Forests is that they provide out of bag OOB samples by construction, so the user does not need to separate training and validation sets form the dataset. 

%users don't have to extract their own validation set and pass it to the feature importance function. 
%
%If we ignore the computation cost of retraining the model, we can get the most accurate feature importance using a brute force drop-column importance mechanism. The idea is to get a baseline performance score as with permutation importance but then drop a column entirely, retrain the model, and recompute the performance score. If we had infinite computing power, the drop-column mechanism would be the default for all RF implementations because it gives us a “ground truth” for feature importance. The importance values could be different between the two strategies, but the order of feature importances should be roughly the same. 
%The importance of a feature is the difference between the baseline and the drop in overall accuracy or $R^2$ caused by permuting the feature. The permutation mechanism is much more computationally expensive than the mean decrease in impurity mechanism, but the results are more reliable.

%Any machine learning model can use the strategy of permuting columns to compute feature importances. 
%This fact is under-appreciated in academia and industry. 

 %The problem is that this mechanism, while fast, does not always give an accurate picture of importance. Furthermore, this common mechanism for computing feature importance is biased; i.e. it tends to inflate the importance of continuous or high-cardinality categorical variables. “the variable importance measures of Breiman's original Random Forest method ... are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories.” \cite{strobl2007bias}. Strobl et al show that “permutation importance over-estimates the importance of correlated predictor variables.”
%The importance value of a feature is the difference between the baseline and the score from the model missing that feature. 
The mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within Random Forest. As a classifier, random forest performs an implicit feature selection, using a small subset of strong variables for the classification only, leading to its superior performance on high dimensional data. 
%
%The risk of this procedure is a potential bias towards correlated predictive variables. 

The "Gini importance" -a computationally efficient approximation to the entropy- is a score that provides a relative ranking of the spectral features and is a by-product in the training of the random forest classifier, that is to say, the feature selection mechanism is embedded in the training algorithm of the classifier.

This is roughly how it works. At each node $\tau$ within the binary trees T of the random forest, the optimal split is computed using the Gini impurity $i(\tau)$, measuring how well a potential split is separating the samples of the two classes in this particular node.

Formally, the decrease in the importance $\delta(i)$ from splitting and sending the sample to two sibling nodes of $\tau$, $\tau_a$ and $\tau_b$, with respective sample fractions $p_a=n_a/n$ and $p_a=n_b/n$ by a threshold $t_\theta$ on variable $\theta$ is defined as:
\begin{equation}
\delta(i)(\tau) = i(\tau) - p_{a}i(\tau_{a})- p_{b}i(\tau_{b})
\label{eq:delatiatu}
\end{equation}
where the the Gini impurity $i(\tau)$ is calculated as: 
\begin{equation}
i(\tau) = 1 -p_{1}^2 - p_{0}^2
\label{eq:iatu}
\end{equation}
where $p_k = \frac{n_k}{n}$ and $n_k$ is the number of samples from the class $k =\{0,1\}$, out of the total n samples.

Performing an exhaustive tree search over all variables $\theta$ available at the node, and over all possible thresholds $t_\theta$, the pair $\theta, t_{\theta}$ leading to a maximal decrease impurity $\delta(i)$ is determined. The decrease in Gini impurity resulting from this optimal split $\delta(i)_{\theta}(\tau, T)$ is recorded and accumulated for all nodes $\tau$ in all trees T in the forest. 

The Gini importance $I_G$ tells us how often any given feature $\theta$ was selected for a split, and how large its overall discriminative value was for the classification problem.

\begin{equation}
I_{G}(\theta) = \sum_T \sum_{\tau} \delta(i)_{\theta}(\tau,T)
\end{equation}

Based on the previous discussion, we can elaborate on the results shown in Figure \ref{fig:rf14}. First we visualize the internal "mechanics" of one decision tree in the random forest, we plot one of the 10,000 decision trees built (Figure \ref{fig:RF_selectorgini})

\begin{figure}[!htb]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/inpaper/tree}
        \caption{Visualization of one of the 10,000 decision trees, the higher the node the moist important. The hyperparameter "maximum depth" has been set to 3 for clarity. If this parameter is not set, the tree will likely overfit the data.} 
        \label{fig:RF_selectorgini}
\end{figure}

We still can group the important features into three groups by importance. A first group with the first 3 most important features, a second group with the next 3 most important features and a last group with the rest (8) features, from the initial 14 features. Table \ref{fig:treeg} shows the accuracy and recall with using this division of groups. 
The group with the three most important features has better accuracy than the other two groups. However, this does not mean that we just need these three features, if we maximize by metrics different from accuracy, for example, recall, the second group obtains better results.


\begin{table}
\caption{Accuracy by training the random forest with three different subsets of the 14 most important features. Group 1 3 best features, Group 2 , next 3 best features and Group 3 remaining 8 features. The forest has a maximum depth equals to 8.}
\label{fig:treeg}
\begin{center}
\begin{tabular}{ |p{4cm}|p{4cm}|p{4cm}|  }

 \hline
 \multicolumn{3}{|c|}{Accuracy using feature groups} \\
 \hline
 Features 1,2,3    & Features 4,5,6 & Features 7,8,9,10,11,12,13,14\\
 \hline
 0.728   & 0.53    &0.683\\
 \hline
\end{tabular}
\end{center}
\end{table}

If we want to take into account the variability in the importance of the features across all the trees in the forest we can weight the Gini importance with the variance: 
\begin{equation}
I(\tau) = G(\tau) * \sigma(\tau)
\end{equation}
In doing so and not surprisingly the group with the largest number of features is the one with best accuracy (Table \ref{fig:rfvar}). Note the accuracy of the first group with $I*(\tau)$ is worse than with Gini.

\begin{table}
\caption{Accuracy with Gini weighted by the variance.}
\label{fig:rfvar}
\begin{center}
\begin{tabular}{ |p{4cm}|p{4cm}|p{4cm}|  }
 \hline
 \multicolumn{3}{|c|}{Accuracy using feature groups} \\
 \hline
 Features 1,2,3    & Features 4,5,6 & Features 7,8,9,10,11,12,13,14\\
 \hline
0.694   & 0.641    &0.72\\
 \hline
\end{tabular}
\end{center}
\end{table}

The importance for each feature on a decision tree is then calculated as:
\begin{equation}
Imp(X_m) = \frac{1}{N_T} \sum_{T} \sum_{t \in T:v(s_{t})=X_{m}} p(t) \delta_i(s_t,t)
\label{eq:gini}
\end{equation}
where $p(t)= N_t/N$ or the the proportion of samples reaching node $t$ and $v(s_t)$ is the variable used in split $s_t$. In words, Equation \ref{eq:gini} tells that every time a split of a node is made on one variable, the impurity criterion for the two descendant nodes is less than the parent node. 
%When using the Gini index as impurity function, this measure is known as the Gini importance or Mean Decrease Gini. 

%Adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.
%The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.For each decision tree, Scikit-learn calculates a nodes importance using Gini Importance, assuming only two child nodes (binary tree):



%0-3  0.728 and 0.302
%4-7 0.53 and 0.5581
%0.683 and 0.2558
\newpage
%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\section{Supplementary materials}

\subsection{The Vallecas Project dataset}
\label{sup:valle}
Table \ref{} describes all the features collected in the \emph{The Vallecas Project}. Only the self-assessed features (marked in bold) are used for feature selection.

The input features can be classified int the following categories: Genetic (APOE and familial AD) + Demographic (), Sleep, Social Engagement, Engagement External World, Diet, Physical Exercise, Cardiovascular, Traumatic Brain Injury, Psychiatric History, Subjective Cognitive Decline and Life Quality.

%YS : table all features


Figure \ref{fig:wrapperlog} shows the results of feature selection method using the Wrapper method called Recursive feature elimination (RFE). RFE fits a model, in this case a logistic regression in order find the optimal number of features. RFE uses cross-validation to score different feature subsets and select the best scoring collection of features. The curve shows that the maximum accuracy $~0.6$ can be obtained with a number of features between 15 and 24 features.  The classifier method used is logistic regression with a L2 regularization maximizing accuracy. Other metrics are possible (recall, ROC or F1) obtaining inferior results. The Inverse of regularization strength $C=1.0$ (smaller values specify stronger regularization) the algorithm to solve the optimization problem is liblinear.

\begin{figure}[!htb]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/edadyrandom/Wrapper_figu}%Wrapper_Log_class_weight_balanced_pena_l2_C_1.0_solver_liblinear_accuracy}
        \caption{Evaluation of the importance of features for conversion to MCI classification using the Recursive feature elimination (RFE) method. The classifier method is logistic regression with L2 regularization (adds the sum of squares of coefficients as penalty term to the loss function) and  maximizing accuracy.} 
        \label{fig:wrapperlog}
\end{figure}

\subsection{Feature importance with Filter and Wrapper methods}
\label{sup:filwra}
\begin{itemize}
	\item Filter: univariate selection methods consisting in finding the variables that contain most information about the target variable. Filter methods use statistical tests (e.g. chi-squared test, Fisher's exact test) or related quantities such as the correlation coefficient to determine, for example, the variables with variance above a certain threshold.
	\item Wrapper: recursive methods that measure the usefulness based on the performance a model(e.g classifier). Differently form filter methods, wrapper methods build prediction models and their performance is the metric used to determine the optimal set of features. Backward Elimination, Forward Selection and Recursive Feature Elimination are examples of wrapper methods. They all have in common that they find the optimum number of features for which the model's accuracy is the highest \cite{Kohavi1997}.
	\item Embedded methods reunites the best of the two worlds (filter and wrapper), similarly to wrapper methods, embedded methods use a learning algorithm to select the optimum set of features, but contrary to wrapper the feature selection decision is integrated in the learning algorithm. Random forest is the most commonly used embedded method for feature selection. Decision trees algorithms select the most informative feature in each learning step in order to split the dataset into smaller and smaller subsets to predict the target value \cite{breiman2001random}. 
\end{itemize}	
We can summarize the above table by stating that \emph{Filter methods} pick up the intrinsic properties of the features estimated via an univariate correlation matrix. 
\emph{Wrapper methods} are more expensive than filter methods because they require classifiers upon which the accuracy is optimized by iterative cross-validation. And finally, in \emph{Embedded methods} the selection of the feature is integrated in the classifier itself rather than being decided from the external accuracy metric. 
This is graphically represented in Figure \ref{}.
%YS figure with boxes and loop to represent this

%https://www.wildcardconsulting.dk/never-do-this-mistake-when-using-feature-selection/
Feature selection is a powerful way of reducing the complexity of a machine learning or statistical model. We use the 1 to 10 rule to select the 14 most important features. The number of subjects with 2 or more visits that convert to MCI (in their latest) visit is 145 and the number of subjects that do not convert are 1034. Thus, according to \emph{one to ten} rule we are interested in selecting the 14 most important features.

We use SelectKBest \cite{scikit-learn}, an algorithm that uses a score function in order to remove all but the highest scoring features. SelectKBest can take as score function the Fisher's test, the Chi square and the mutual information. 
Thus, SelectKBest with $\tilde{\chi}^2$ as a score function computes the  $\tilde{\chi}^2$ statistic between each input feature of and the target feature. A large value will mean the feature X is non-randomly related to y, and so likely to provide important information. From the initial 92 features only 14 features will be retained.
%['apoe', 'familial_ad', 'renta', 'sexo', 'nivel_educativo',
       % 'anos_escolaridad', 'sdestciv', 'numhij', 'sdvive', 'sdocupac',
       % 'sdresid', 'sdtrabaja', 'sdeconom', 'sdatrb', 'lat_manual', 'pabd',
       % 'peso', 'talla', 'audi', 'visu', 'imc', 'sue_con', 'sue_dia', 'sue_hor',
       % 'sue_man', 'sue_mov', 'sue_noc', 'sue_pro', 'sue_rec', 'sue_ron',
       % 'sue_rui', 'sue_suf', 'relafami', 'relaamigo', 'relaocio_visita1',
       % 'rsoled_visita1', 'a01', 'a02', 'a03', 'a04', 'a05', 'a06', 'a07',
       % 'a08', 'a09', 'a10', 'a11', 'a12', 'a13', 'a14', 'alaves', 'alcar',
       % 'aldulc', 'alemb', 'alfrut', 'alhuev', 'allact', 'alleg', 'alpan',
       % 'alpast', 'alpesblan', 'alpeszul', 'alverd', 'ejfisicototal', 'hta',
       % 'glu', 'lipid', 'tabac', 'cor', 'arri', 'card', 'tir', 'ictus', 'tce',
       % 'depre', 'ansi', 'scd_visita1', 'act_aten_visita1', 'peorotros_visita1',
       % 'act_mrec_visita1', 'act_expr_visita1', 'eqm06_visita1',
       % 'eqm07_visita1', 'eqm81_visita1', 'eqm82_visita1', 'eqm83_visita1',
       % 'eqm85_visita1', 'eqm09_visita1', 'eqm10_visita1', 'eq5ddol_visita1',
       % 'valfelc2_visita1', 'eq5dsalud_visita1']

It ought to be remarked that the Fisher and the $\tilde{\chi}^2$ methods estimate the degree of linear dependency between two random variables. Mutual information, on the other hand, can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.


\begin{table}[h!]
  \begin{center}
    \caption{Table with the 14 most important features using the SelectKBest using the Fisher's test and the $\tilde{\chi}^2$ as score functions.}
    \label{tab:selectk}
    \begin{tabular}{l|r|r} % <-- Changed to S here.
      %\textbf{Value 1} & \textbf{Value 2} & \textbf{Value 3}\\
      Rank & $\tilde{\chi}^2$ test & F test\\
      \hline
      1 & scd & a13\\
      2 &  apoe & eqm10\\
      3 &  sdvive & apoe\\
      4 &  tir& scd \\
      5 &  a13 & aldulc\\
      6 &  aldulc& sdvive\\
      7 &  eqm10& imc\\
      8 &  imc& relaamigo\\
      9 &  sue hor& eq5dsalud\\
      10 & eqm83& a08\\
      11 & relaamigo& sdresid\\
      12 & sue rui& a14\\
      13 & ictus& a01\\
      14 & anos escolaridad& \\
    \end{tabular}
  \end{center}
\end{table}

%Fisher no apoe ['a13', 'eqm10_visita1', 'scd_visita1', 'eqm83_visita1', 'aldulc','sdvive', 'imc', 'relaamigo', 'eq5dsalud_visita1', 'a08', 'sdresid','a14', 'a01', 'audi']
% Chio2 no apoe ['scd_visita1', 'sdvive', 'tir', 'a13', 'aldulc', 'eqm10_visita1', 'imc','sue_hor', 'eqm83_visita1', 'relaamigo', 'sue_rui', 'ictus','anos_escolaridad', 'numhij']

%MI ['sue_mov', 'nivel_educativo', 'card', 'eqm81_visita1', 'sdresid','alaves', 'eqm09_visita1', 'a09', 'familial_ad', 'a07', 'sue_rui','ejfisicototal', 'sue_ron', 'allact']


Figure with the most improtant features selected by the SelectKBest ($k=14$, 10:1 rule of thumb), using Fisher's and $\tilde{\chi}^2$  tests to study the importance of the features: ANOVA \ref{fig:HeatmapTopF_f_classif-b}, Chi2 \ref{fig:HeatmapF_chi2-b}.
% and MI \ref{fig:HeatmapTopF_mutual_info_classif-b}.
In the axis are shown the 14 features selected using univariate Feature Selection method. Previously,  the features with low variance $<(80\%)$ were removed.
%REMOVED for low var u'sdhijos', u'act_orie_visita1', u'act_prax_visita1',u'act_ejec_visita1', u'act_comp_visita1', u'act_visu_visita1',u'eqm84_visita1', u'eqm86_visita1', u'eq5dmov_visita1'],

\begin{figure}[!htb]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/inpaper/HeatmapTopF_chi2}
        \caption{Heatmap of Pearson's correlation for features selected with the Chi2 test SelectKBest \cite{scikit-learn} (only a correlation larger than +-.10 shown)..} 
        \label{fig:HeatmapF_chi2-b}
\end{figure}

\begin{figure}[!htb]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/HeatmapTopF_f_classif}
        \caption{Heatmap of Pearson's correlation for features selected with the Fisher's test SelectKBest \cite{scikit-learn} (only a correlation larger than +-.10 shown)..} 
        \label{fig:HeatmapTopF_f_classif-b}
\end{figure}

%YS: include MI ? if not explain why
% \begin{figure}[!htb]
%         \centering
%         \includegraphics[keepaspectratio, width=\linewidth]{figures/HeatmapTopF_mutual_info_classif}
%         \caption{Heatmap of Pearson's correlation for features selected with the Mutual Information test SelectKBest \cite{scikit-learn} (only a correlation larger than +-.15 shown)..} 
%         \label{fig:HeatmapTopF_mutual_info_classif-b}
% \end{figure}


\textbf{Wrapper:Recursive feature elimination}

Recursive feature elimination works by recursively eliminate features to build a model with the  remaining features. For example, RFE uses logistic regression to select the top N features. 
%Wrapper methods estimate the usefulness of each feature before to include it in the classifier, only those features that are identified as useful are included all the rest are removed. 
Wrapper method optimize the classifier performance, on the cons side they are more expensive than filter methods.
Wrapper methods use a machine learning algorithm and uses its performance as an evaluation criterion, basically the idea is to incrementally feed features to the model in order to determine the set of features that yield a satisfactory model performance (e.g. accuracy of 0.85 or more). RFE uses the accuracy metric to rank the features, the goal is to find the optimum number of features for which the accuracy is the highest. A loop starts with one feature and incrementally add from 1 to M, the final selection of features provides the best possible accuracy among all possible combination of features.

Wrapper methods are computationally expensive because they need to train a new model for each subset of features.
Wrapper methods can be divided into exhaustive search and non exhaustive search. 
 %heautristic search and random search. 
 Exhaustive search (Breadth First Search BFS) enumerates all possible feature combinations $O(2^n)$, it is rarely used because the complexity $O(2^n)$,. Non-exhaustive search (eg heuristic, random) saves time by cutting branches. The common classes of non-exhaustive algorithms are Sequential Forward Selection SFS and Sequential Backward Selection SBS). SFS starts form an empty set, each time a feature is selected the evaluation metric improves. SBS, on the other hand, starts from the universal set of features and deletes a feature each time (the features that deteriorates the evaluation metric). Both BFS and SFS are greedy-algorithms and therefore they likely fall into local minimum. Wrapper methods usually provide the best set of features for a particular model eg OLS, logistic regression) but they are very computationally intensive since we need to train a new model each time (iteration of set of features. (Random forest is not a model that can be done ion wrapper)


Wrapper: RFE, GA

Maximizing SVC (C=1 or 0.01) by accuracy does not make sense, I get Optimal number of features : 1!!

%Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.
RFECV performs RFE in a cross-validation loop to find the optimal number of features


\subsection{Random Forest}

Random forest is a bagging algorithm because it tries to "fix" the reliability problem existing in the decision tree model. Decision trees are unreliable or unstable in the sense that small changes in the input produce may produce very different decision trees. This is where bagging comes from, we can create a robust model through bagging, that is, create a multiset of data by resampling the original dataset. Each tree will deal with a different set of the data called the bootstrap sample chosen at random with replacement. This means that a bootstrap sample may have missing instances and also repeated instances. We do this N times (one for each tree). Importantly, there is not only randomness in picking from the multiset for each tree but also there is randomness on the features from which to decide to split or not. Thus, the Random Forest algorithm can introduce extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in greater tree diversity, trading a higher bias for a lower variance [15]. Once the random forest is trained, the overall prediction is done weighting votes.
Not surprisingly, random forest inherit many of the strengths and weaknesses of decision trees. Pros: widely used with excellent performance on a variety of problems, easily parallelized, does not require careful normalization of features. Cons: it may not be good for very high dimensional problems (text classifiers).

\textbf{Out of bag error}
%https://datascience.stackexchange.com/questions/13151/randomforestclassifier-oob-scoring-method
In general the performance of classifiers is measured using accuracy (number of correctly classified instances divided by the total number of instances). However, from the training data we can get a better approximation of the expected error from our classifier when we are using ensemble learning or bagging techniques. 
The OOB error is the metric of the accuracy of examples using all th trees in the RF for which it was omitted during training. Thus, in a way is a sort of semi-testing instance. With OOB you can get a sense of how the classifier can generalize.
The oob accuracy is a better metric by which to evaluate the performance of your model rather than just the score this can be done only with bagging models.

\textbf{Permutation}
Feature importance is available for more than just linear models. Most random Forest (RF) implementations also provide measures of feature importance. In fact, the RF importance technique we'll introduce here (permutation importance) is applicable to any model, though few machine learning practitioners seem to realize this. Permutation importance is a common, reasonably efficient, and very reliable technique. It directly measures variable importance by observing the effect on model accuracy of randomly shuffling each predictor variable. This technique is broadly-applicable because it doesn't rely on internal model parameters, such as linear regression coefficients (which are really just poor proxies for feature importance) \cite{breiman2001statistical}. 
One of Breiman's issues involves the accuracy of models. The more accurate our model, the more we can trust the importance measures and other interpretations. Measuring linear model goodness-of-fit is typically a matter of residual analysis. The problem is that residual analysis does not always tell us when the model is biased. Breiman quotes William Cleveland, “one of the fathers of residual analysis,” as saying residual analysis is an unreliable goodness-of-fit measure beyond four or five variables. 

%Gini or mean decrease in impurity https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined




%Embedded: LASSO regularization, decision tree.
% YS Figure with a loop that i saw somewhere 
%https://www.quora.com/What-is-the-univariate-correlation-matrix-Is-it-different-from-the-Pearson-correlation-analysis
%In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. @ 1997 Elsevier Science B.V.
%f you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features and this is what biases the performance analysis. %biased estimator is the one where feature selection is performed prior to cross-validation, the unbiased estimator is the one where feature selection is performed independently in each fold of the cross-validation
%https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/
Machine learning works on a simple rule – if you put garbage in, you will only get garbage to come out. By garbage here, I mean noise in data.

This becomes even more important when the number of features are very large. You need not use every feature at your disposal for creating an algorithm. You can assist your algorithm by feeding in only those features that are really important. 
Top reasons to use feature selection are:

    It enables the machine learning algorithm to train faster.
    It reduces the complexity of a model and makes it easier to interpret.
    It improves the accuracy of a model if the right subset is chosen.
    It reduces overfitting.

Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods.
Difference between Filter and Wrapper methods

The main differences between the filter and wrapper methods for feature selection are:

    Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.
    Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.
    Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.
    Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.
    Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods.

Not only we have improved the accuracy but by using just 20 predictors instead of 100, we have also:

    increased the interpretability of the model.
    reduced the complexity of the model.
    reduced the training time of the model.

%https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3
Decision trees learn how to best split the dataset into smaller and smaller subsets to predict the target value. The condition is represented as the “leaf” (node) and the decision as “branches” (edges). This splitting process continues until no further gain can be made or a preset rule is met, e.g. the maximum depth of the tree is reached.

%ID3 creates a multi way tree each node can have two or more edges: Builds fast and short tree, Whole dataset is searched to create tree. However, Data may be over-fitted or over classified, if a small sample is tested, does not handle numeric attributes.

%CART stands for Classification and Regression Trees. The algorithm creates a binary tree — each node has exactly two outgoing edges. For classification, Gini impurity or twoing criterion can be used. For regression, CART introduced variance reduction using least squares. CART can easily handle both numerical and categorical variables, identify the most significant variables and eliminate non-significant ones, handle outliers. However may have unstable decision tree.
 
Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. For each decision tree, Scikit-learn calculates a nodes importance using Gini Importance, assuming only two child nodes (binary tree).
%https://chrisalbon.com/machine_learning/trees_and_forests/feature_selection_using_random_forest/
Feature Selection Using Random Forest

Goal: we want a way to create a model that only includes the most important features. Benefits, at least 3:
make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called “feature selection.”Random Forests are often used for feature selection in a data science workflow. The reason is because the tree-based strategies used by random forests naturally ranks by how well they improve the purity of the node. This mean decrease in impurity over all trees (called gini impurity). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features \cite{breiman2001random}.

%https://datascience.stackexchange.com/questions/26283/how-can-i-fit-categorical-data-types-for-random-forest-classification
% to convert the categorical features into numeric attributes. A common approach is to use one-hot encoding, but that's definitely not the only option. If you have a variable with a high number of categorical levels, you should consider combining levels or using the hashing trick. Sklearn comes equipped with several approaches (check the "see also" section): One Hot Encoder and Hashing Trick
%%
%% End of file `elsarticle-template-1-num.tex'.
%%%%%%% MISC %%%%%
\subsection{How much training data do you need?}
%https://medium.com/@malay.haldar/how-much-training-data-do-you-need-da8ec091e956
%https://towardsdatascience.com/how-do-you-know-you-have-enough-training-data-ad9b1fd679ee
How much training data do you need?
The correct answer is: it depends. It depends on the task you are trying to perform, the performance you want to achieve, the input features you have, the noise in the training data, the noise in your extracted features, the complexity of your model and so on. So the way to find out the interaction of all these variables is to train your model on varying amounts of training data and plot learning curves.But this requires you to already have some decent amount of training data to construct interesting plots

First, automatically generate a lot of logistic regression problems. The first observation is that the f-score curves don’t vary as the parameters scale. This is expected given the models are linear and it’s good to see that some hidden non-linearity doesn’t creep in.The second observation is that when the ratio of training samples to model parameters is 10:1, the f-score lands in the vicinity of 0.85 which we take as the definition of a well performing model.
%https://github.com/Malay-Haldar/Tensorflow-Projects/blob/master/training_data_exploration.py
This leads us to the rule of 10, namely the amount of training data you need for a well performing model is 10x the number of parameters in the model.The rule of 10 transforms the problem of estimating the amount of training data required to knowing the number of parameters in the model.For linear models such as logistic regression, the number of parameters equal the number of input features since the model assigns a parameter corresponding to each feature. However there are complications:
\begin{itemize}
	\item features may be sparse, so counting the number of features may not be straightforward.
	\item Due to regularization and feature selection techniques a lot of features may be discarded
\end{itemize}

Neural networks pose a different set of problems than linear models like logistic regression. The problem is the relationship between the parameters in a neural network is no longer linear, so the empirical study we did based on logistic regression doesn’t really apply anymore. In such cases you can treat the rule of 10 as a lower bound to the amount of training data needed.
the rule of 10 seem to work across a wide range of problems, including shallow neural nets. 

In statistics, the one in ten rule is a rule of thumb for how many predictor parameters can be estimated from data when doing regression analysis while keeping the risk of overfitting low. 

The rule states that one predictive variable can be studied for every ten events and an event is the size of the smallest class. For example, if a sample of 200 patients are studied and 20 patients die during the study (so that 180 patients survive), then 2 pre-specified predictors can reliably be fitted to the total data (2 to 20 points in the minority class). Similarly, if 100 patients die during the study, then 10  pre-specified predictors can be fitted reliably. The more element sin the minority cass the more predictors I need. Thus, for the Vallecas dataset (if we were doing regression analysis) 1000 in year 1, with $10\%$ converters to MCI or 100 the index should be around 10.

More restrictive, that is less parameters, rules are the "one in 20 rule", indicating the need for shrinkage of regression coefficients, and a "one in 50 rule" for stepwise selection with the default p-value of $5\%$.
Other studies, however, show that the one in ten rule may be too conservative as a general recommendation and that five to nine events per predictor can be enough, depending on the research question. Thus for 5 to 1 in the Vallecas we will be talking about 20 parameters.

More recently, a study has shown that the ratio of events per predictive variable is not a reliable statistic for estimating the minimum number of events for estimating a logistic prediction model.[8] Instead, the number of predictor variables, the total sample size (events + non-events) and the events fraction (events / total sample size) can be used to calculate the expected prediction error of the model that is to be developed \cite{van2018sample}. One can then estimate the required sample size to achieve a expected prediction error that is smaller than a predetermined allowable prediction error value.

Alternatively, three requirements for prediction model estimation have been suggested: the model should have a global shrinkage factor of $ \geq.9$, an absolute difference of $\leq .05$ in the model's apparent and adjusted Nagelkerke R2, and a precise estimation of the overall risk or rate in the target population.[10] The necessary sample size and number of events for model development are then given by the values that meet these requirements.

%https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/
In Computer Vision: For image classification using deep learning, a rule of thumb is 1,000 images per class,

% power and saple sze
%http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html

The Vapnik-Chevronenkis (VC) dimension is a measure of the complexity of a model; the more complex the model, the higher its VC dimension. Thus we can specify the training data size in terms of the VC dimension.
VC dimension is the cardinality of the largest set of points that the algorithm can shatter. 

The VC of the linear classifier is precisely 3 because we can find examples of 4 points that can NOT be accurately separated by a line. It turns out that the training data size, N, is a function of VC. The amount of data needed for learning depends on the complexity of the model. A side effect of this is the well-known voracity of neural networks for training data, given their significant complexity \cite{juba2019precision}

The Estimation of training data size (N) from the VC dimension is
\begin{equation}
N = F\frac{VC + \log(1/d)}{\epsilon}
\end{equation}
where d is the probability of failure and epsilon is the learning error

A Methodology to Determine Training Data Size in Classification:  learning curve, which in general is a plot of error versus training data size. In classification, we typically use a slightly different form of the learning curve; it is a plot of classification accuracy versus training data size. he methodology for determining training data size is straightforward: Determine the exact form of the learning curve for your domain, and then, simply find the corresponding point on the plot for your desired classification accuracy. 

No imbalance-correcting technique (under-sampling, over-sampling and ensemble learning) can match adding more training data when it comes to measuring precision and recall.
%https://www.dataquest.io/blog/learning-curves-machine-learning/
Learning curves:

\subsection{Feature Selection}
%https://forums.fast.ai/t/feature-importance-of-random-forest-vs-xgboost/17561
The FA of Random Forest (Bagging Ensemble of trees) vs XGBoost (Boosting Ensemble of trees) is the same.
%https://explained.ai/rf-importance/index.html
The scikit-learn Random Forest feature importance and R's default Random Forest feature importance strategies are biased. To get reliable results in Python, use permutation importance
\paragraph*{Introduction to Feature Importance}
Training a model that accurately predicts outcomes is great, but most of the time you don't just need predictions, you want to be able to interpret your model. Feature importance is the most useful interpretation tool, and data scientists regularly examine model parameters (such as the coefficients of linear models), to identify important features. 





You learned about 4 different automatic feature selection techniques:  univariate selection, Recursive Feature Elimination, PCA and Feature Information.
%https://machinelearningmastery.com/feature-selection-machine-learning-python/
Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.
Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.
Three benefits of performing feature selection before modeling your data are:
\begin{itemize}
\item reduce overfitting
\item improves Accuracy
\item reduces training time

\end{itemize}

%scikit-learn.org/stable/modules/feature_selection.html
%The classes in the sklearn.feature_selection module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.


Univariate Selection (Statistical tests) d:  SelectKBest(score func=chi2, k=4)

Recursive Feature Elimination: works by recursively removing attributes and building a model on those attributes that remain. For example, use RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent.
%scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE

Principal Component Analysis:  a data reduction technique. You can choose the number of dimensions or principal component in the transformed result.

Feature Importance: Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features. (ExtraTreesClassifier)

%https://medium.com/data-science-journal/how-to-measure-feature-importance-in-a-binary-classification-model-d284b8c9a301
Training a model on such a huge table (all the features) is not a good idea. You really run the risk of collinearity (i.e. correlations between variables). So we have to choose the best set of variables to use in order to make our model learn properly.
Our goal is to increase the predictive power of our model against our binary target, so we must find those variables that are strongly correlated with it. Remember: information is hidden inside the dataset and we must provide all the necessary conditions to make our model extract it. So we have to prepare data before the training phase in order to make the model work properly.
Pearson correlation coefficient is not flawless, however. It only measures linear correlation and our variables couldn’t be linearly correlated. For the categorical variables, there’s no Pearson correlation coefficient, but we can use another great discovery of Pearson, which is the chi-square test.


\section{Materials and Methods}
\label{S:2}
%\textbf{Filter methods}: information gain, chi-square test, fisher score, correlation coefficient, variance threshold (compute the variance of each feature, and we select the subset of features based on a user-specified threshold. E.g., “keep all features that have a variance greater or equal to x”). We assume that features with a higher variance may contain more useful information, but note that we are not taking the relationship between feature variables or feature and target variables into account, which is one of the drawbacks of filter methods.


%https://towardsdatascience.com/feature-engineering-using-machine-learning-on-large-financial-datasets-8584dacd3a4d
Feature Important using random forests: data cleaning to remove the non essential data which doesn’t contribute in any meaningful way to our target feature.
%https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b
%https://sebastianraschka.com/faq/docs/feature_sele_categories.html
Feature selection can be done in multiple ways but there are broadly 3 categories of it: filter, warapper and embedded. Filter method not taking the relationship between feature variables or feature and target variables into account
Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance. So, wrapper methods are essentially solving the “real” problem (optimizing the classifier performance), but they are also computationally more expensive compared to filter methods due to the repeated learning steps and cross-validation. Embedded methods, are quite similar to wrapper methods since they are also used to optimize the objective function or performance of a learning algorithm or model. The difference to wrapper methods is that an intrinsic model building metric is used during learning

Filter methods: information gain, chi-square test, fisher score, correlation coefficient, variance threshold (compute the variance of each feature, and we select the subset of features based on a user-specified threshold. E.g., “keep all features that have a variance greater or equal to x”). We assume that features with a higher variance may contain more useful information, but note that we are not taking the relationship between feature variables or feature and target variables into account, which is one of the drawbacks of filter methods.
Wrapper: RFE, GA
Embedded: LASSO regularization, decision tree.


Filtering uses Pearson or other correlation measures.  One of the assumptions of linear regression is that the independent variables need to be uncorrelated with each other. If these variables are correlated with each other, then we need to keep only one of them and drop the rest. 
Warapper method needs one machine learning algorithm and uses its performance as evaluation criteria:  you feed the features to the selected Machine Learning algorithm and based on the model performance you add/remove the features.There are different wrapper methods such as Backward Elimination, Forward Selection, Bidirectional Elimination and RFE.

%paso de backward do RFE
Backward Elimination: we feed all the possible features to the model at first. We check the performance of the model (eg OLS) and then iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range. The performance metric used here to evaluate feature performance is pvalue. If the pvalue is above 0.05 then we remove the feature, else we keep it.

RFE (Recursive Feature Elimination): uses accuracy metric to rank the feature according to their importance. The RFE method takes the model to be used and the number of required features as input (eg LinearRegression model with 7 features ).Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to M total features. We then take the one for which the accuracy is highest. We finally get the optimum number of features with the score for the optimal number of features.
Sequential Feature Selection: greedy search algorithm that attempts to find the “optimal” feature subset by iteratively selecting features based on the classifier performance. We start with an empty feature subset and add one feature at the time in each round (more expensive than a filter approach such as the variance threshold)

Embedded: Regularization, If the feature is irrelevant, lasso penalizes it’s coefficient and make it 0. L1 (or LASSO) regression for generalized linear models can be understood as adding a penalty against complexity to reduce the degree of overfitting or variance of a model by adding more bias. We add a penalty term directly to the cost function, regularized cost = cost + regularization penalty. we can induce sparsity through this L1 vector norm


(PCA is feature extraction it maps features into a new feature space)
\section{Appendix}
\label{se:appe}
%https://medium.com/@cxu24/common-methods-for-feature-selection-you-should-know-2346847fdf31
The goal of a feature selection algorithm is to find the optimal feature subset using an evaluation measure.  The choice of evaluation metric distinguish the three main strategies of feature selection algorithms: the wrapper strategy, the filter strategy, and the embedded strategy

Wrapper methods train a new model for each subset and use the error rate of the model on a hold-out set to score feature subsets. Wrapper methods are subdivided into exhaustive search, heuristic search, and random search.
Exhaustive search like BFS (Breadth First Search) enumerates all possible feature combinations, rarely used $O(2^n)$. Non-exhaustive search  saves time by cutting off branches.
Heuristic search has SFS (Sequential Forward Selection) and SBS (Sequential Backward Selection). SFS starts from an empty set. Each time a feature x is added to the feature subset X so that the evaluation metric could be optimized. SBS, on the other hand, starts from the universal set and deletes a feature x each time. Both SFS and SBS are greedy algorithms that likely fall into the local optimum. Wrapper methods usually provide the best performing feature set for a particular type of model. However, they are very computationally intensive since for each subset a new model needs to be trained.

Filter Methods: is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. 

Feature/Response  Cont      Categ
Cont 			  Pearson	LDA
Cat               Anova     Chi 
LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable
Importantly, filter methods do not remove multicollinearity

Instead of the error rate, filter methods use evaluation criteria from the intrinsic connections between features to score a feature subset. Filter methods are independent to the type of predictive model. The result of a filter would be more general than a wrapper. Common measures have four types: distance metrics, correlation, mutual information, and consistency metrics. Mutual information is more general than correlation coefficient. It quantifies the “the amount of information” obtained about one feature through the other feature. Mutual information determines how similar the joint distribution p(X, Y) is to the products of factored marginal distribution p(X)p(Y). 
A favorite consistency metrics is chi-square, we calculate chi-square between each feature and the target. The intuition is that features that are independent to the target are uninformative.

Embedded Methods

In embedded techniques, the feature selection algorithm is integrated as part of the learning algorithm.The most typical embedded technique is decision tree algorithm. Decision tree algorithms select a feature in each recursive step of the tree growth process and divide the sample set into smaller subsets. The more child nodes in a subset are in the same class, the more informative the features are. The process of decision tree generation is also the process of feature selection. ID3, C4.5, and CART are all common decision tree algorithms.
Other exemplars of this approach are the LASSO with the L1 penalty and Ridge with the L2 penalty for constructing a linear model. These two methods shrink many features to zero or almost zero.


%https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder
%https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621
 we can’t have text in our data if we’re going to run any kind of model on it. to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.
So all we have to do, to label encode the first column, is import the LabelEncoder class from the sklearn library, fit and transform the first column of the data, and then replace the existing text data with the new encoded data
%from sklearn.preprocessing import LabelEncoder
%labelencoder = LabelEncoder()
%x[:, 0] = labelencoder.fit_transform(x[:, 0]) 0 column contains text
%but if we transform categorical (eg country names) into number, numbers have a cardinality, but countries dont! that is there is no order implicit in countries to remove this annoyance we can use OneHotEncoder.
%label encoding, we might confuse our model into thinking that a column has data with some kind of order or hierarchy. What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value. In our example, we’ll get three new columns, one for each country France, Germany, and Spain. Sparsity.


%from sklearn.preprocessing import OneHotEncoder
%onehotencoder = OneHotEncoder(categorical_features = [0])
%x = onehotencoder.fit_transform(x).toarray()
%%%%% END MISC %%%%%%